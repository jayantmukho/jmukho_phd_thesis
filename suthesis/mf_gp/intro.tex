Most computational or experimental analysis techniques provide realizations of a quantity of interest (QoI) at discrete points in the domain of interest. 
Each analysis, referred to as a function evaluation, has a monetary and computational cost. 
In practice, higher-fidelity function evaluations are more costly than lower-fidelity ones.
If the converse were true, there would be no reason to use a lower-fidelity analysis in place of a higher-fidelity one.
Fidelity here refers to how closely an analysis technique mirrors real-life physics. 

In theory, the discretization of the domain can be fine enough that it results in a nearly continuous representation of the QoI.
Additionally, using only the highest-fidelity analysis techniques would minimize the uncertainty or error in the QoI predictions. 
In reality, this is not tractable. 
Often cost minimization is a priority. 
This constraint equates to representing QoIs as sparsely as possible and lower-fidelity analysis techniques replacing higher-fidelity ones wherever valid.  

Numerous statistical methods that use these discrete realizations to create continuous representations of the QoI, have been developed.
These representations, called surrogate models, assume that the underlying function of interest varies smoothly between the discrete data points.
It allows us to squeeze the most out of the available, limited function evaluations.
Popular surrogate modeling techniques include radial basis functions \cite{park1991universal}, Gaussian processes (GP) \cite{krige1951statistical,matheron1963principles,rasmussen_gaussian_2006}, stochastic collocation \cite{loeven2007probabilistic}, and polynomial chaos expansions (PCE) \cite{oladyshkin2012data,blatman2011adaptive}.

Of these, the use of multi-fidelity data has been developed for GP \cite{kennedy_predicting_2000,gratiet_multi-fidelity_nodate} and, more recently, for PCE \cite{ng2012multifidelity, palar2018global}.
Both methods handle the multi-fidelity data by using correction terms trained on the difference between the low-fidelity and high-fidelity data.
GP have a slight advantage in this regard as they use an additive and a multiplicative term in the correction, whereas PCE only uses an additive term. 
PCE have a distinct advantage in sensitivity analyses as Sobol indices can be directly post-processed \cite{sudret2008global,crestaux2009polynomial}.
On the other hand, GP can handle uncertain inputs and directly estimate the error in its modeling.
This feature can be used for adaptive sampling techniques that suggest additional function evaluations to reduce the uncertainty in the model \cite{xu2011adaptive}.
For this work, GP are the surrogate model of choice due to its advanced handling of multi-fidelity data that have associated uncertainties and the direct estimation of uncertainty in its predictions.

This chapter introduces the fundamental equations of GP regression used to handle single-fidelity data that can have uncertainties associated with it.
It builds upon this by introducing multi-fidelity GP regression that combines data from different sources to build a single, superior surrogate model for the QoI.
These multi-fidelity GP equations are then used to create probabilistic aerodynamic databases representing the performance characteristics of a model aircraft, the NASA CRM. 
The benefits of using multi-fidelity data vs. single-fidelity data are emphasized as well.